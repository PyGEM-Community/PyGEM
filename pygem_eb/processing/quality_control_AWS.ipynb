{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code\n",
    "GOAL: take in RAW, not quality controlled data and clean it up. This does NOT include interpolation of missing data points, but does include:\n",
    "1. Compare to sensor health data or logs of known failure periods and remove data in those windows\n",
    "2. Plausible value check. Removes any values that are beyond standard range for each variable\n",
    "3. Remove noise. Higher-level processing but could follow USGS methods\n",
    "4. Compare site-best. If multiple sensors exist for a given data variable, compare values and fill gaps \n",
    "5. Combine data files. If data is separated into multiple datafiles, resample to get on the same time basis (can resample to hourly here, or the lowest resolution of what is available). Combine files with consistent naming into one dataframe for checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_fp = '~/research/climate_data/AWS/Raw/'\n",
    "data_fns = ['SouthGlacier_AWS_HalfHourData.csv','SouthGlacier_AWS_FiveMinData.csv','SouthGlacier_AWS_HealthData.csv']\n",
    "time_vn = 'TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2729/789637400.py:1: DtypeWarning: Columns (11,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(AWS_fp+data_fns[1],index_col=time_vn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['RECORD', 'WS_ms_S_WVT', 'WindDir_D1_WVT', 'WindDir_SD1_WVT',\n",
       "       'WS_ms_Max', 'AirTC', 'NR_Wm2_Avg', 'CNR_Wm2_Avg', 'RH', 'SWin_Wm2_Avg',\n",
       "       'SWout_Wm2_Avg', 'cnr4_T_C_Avg', 'short_up_Avg', 'short_dn_Avg',\n",
       "       'long_up_corr_Avg', 'long_dn_corr_Avg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(AWS_fp+data_fns[1],index_col=time_vn)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename variables to have consistency\n",
    "\n",
    "*** How to handle when there are multiple columns with the same data? i.e. two SWin terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {'temp':['site_temp_USGS','temperature','Tair_aws','temp','TA_2.0m','T','AirTC'],\n",
    "            'tp':['Precip_Weighing_Incremental','precipitation','Ptotal_aws','tp','P','Rain_mm_tot'],\n",
    "            'rh':['RelHum','RH','rh','rH','RH_aws','RH_2.0m'],\n",
    "            'SWin':['RadiationIn','SWin','SWin_aws','SW_IN','short_dn_Avg'],\n",
    "            'SWout':['RadiationOut','SWout','SWout_aws','SW_out','SW_OUT','short_up_Avg'],\n",
    "            'LWin':['LWRadiationIn','LWin','LWin_aws','LW_in','LW_IN'],\n",
    "            'LWout':['LWRadiationOut','LWout','LWout_aws','LW_OUT'],\n",
    "            'wind':['WindSpeed','wind','Wind','ws_aws','WS','WS_ms_S_WVT'],\n",
    "            'winddir':['VecAvgWindDir','WindDir','Winddir','winddir','WD','WindDir_D1_WVT'],\n",
    "            'sp':['barom','sp','press','Press_aws','Barom','BP'],\n",
    "            'tcc':['cloud_fraction','tcc','CCF','CCF_aws']}\n",
    "# RENAMING\n",
    "drop_vars = []\n",
    "all_vars = ['temp','tp','rh','SWin','SWout','LWin','LWout','wind','winddir','sp','tcc']\n",
    "for var in df.columns.to_numpy():\n",
    "    renamed = False\n",
    "    for var_check in all_vars:\n",
    "        if var in names[var_check]:\n",
    "            df = df.rename(columns={var:var_check})\n",
    "            all_vars.remove(var_check)\n",
    "            renamed = True\n",
    "    if not renamed:\n",
    "        drop_vars.append(var)\n",
    "if len(drop_vars) > 0:\n",
    "    print('Variables were not renamed, including:')\n",
    "    print(drop_vars)\n",
    "else:\n",
    "    drop_vars = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check datatypes to sort out random strings or non-float values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2729/865519202.py:2: DtypeWarning: Columns (11,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(AWS_fp+fn,index_col=time_vn)\n"
     ]
    }
   ],
   "source": [
    "for fn in data_fns:\n",
    "    df = pd.read_csv(AWS_fp+fn,index_col=time_vn)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensor malfunctions to remove time periods where the sensor is known to be malfunctioning, according to some indicator (panel temperature, voltage, etc.) and healthy limit for said indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    3085\n",
      "True      251\n",
      "Name: Panel_Temp_Max, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load in health dataset and specify bounds to remove datapoints\n",
    "health_df = pd.read_csv(AWS_fp+health_fn,index_col=time_vn)\n",
    "indicator = 'Panel_Temp_Max'\n",
    "healthy_limit = 10\n",
    "unhealthy_idx = health_df[indicator] > healthy_limit\n",
    "# print(unhealthy_idx)\n",
    "print(unhealthy_idx.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plausible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define boundaries for each variable\n",
    "bounds = {'temp':[-40,30],'precip':[0,60],'wind':[0,75],'winddir':[0,360],\n",
    "            'sp':[50,110],'sw':[0,1600],'lw':[-100,400],'rh':[0,100],'tcc':[0,100]}\n",
    "units = {'temp':'C','precip':'mm hr-1','wind':'m s-1','winddir':'deg',\n",
    "            'sp':'kPa','sw':'W m-2','lw':'W m-2','rh':'%','tcc':'%'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2729/2546900691.py:6: DtypeWarning: Columns (11,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_load = pd.read_csv(AWS_fp+fn,encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# def merge_files(fns):\n",
    "fns = [data1_fn,data2_fn]\n",
    "df = pd.read_csv(AWS_fp+fns[0],index_col=time_vn)\n",
    "timestep_original = pd.to_datetime(df.index[1]) - pd.to_datetime(df.index[0])\n",
    "for fn in fns[1:]:\n",
    "    df_load = pd.read_csv(AWS_fp+fn,encoding='ISO-8859-1')\n",
    "    timestep_load = pd.to_datetime(df_load.index[1]) - pd.to_datetime(df_load.index[0])\n",
    "    if timestep_load < timestep_original:\n",
    "        print(timestep_load.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stats(fn,header,droprows):\n",
    "    df = pd.read_csv(AWS_fp+fn,header=header,encoding = 'ISO-8859-1')\n",
    "    df = df.drop(droprows,axis=0)\n",
    "    df = df.set_index(time_vn)\n",
    "    for column in df.columns:\n",
    "        print(column,'Nonzero count:',df[column].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2729/916621188.py:2: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(AWS_fp+fn,header=header,encoding = 'ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECORD Nonzero count: 956668\n",
      "WS_ms_S_WVT Nonzero count: 956668\n",
      "WindDir_D1_WVT Nonzero count: 956668\n",
      "WindDir_SD1_WVT Nonzero count: 956668\n",
      "WS_ms_Max Nonzero count: 956668\n",
      "AirTC Nonzero count: 956668\n",
      "NR_Wm2_Avg Nonzero count: 956668\n",
      "CNR_Wm2_Avg Nonzero count: 191532\n",
      "RH Nonzero count: 956668\n",
      "SWin_Wm2_Avg Nonzero count: 457951\n",
      "SWout_Wm2_Avg Nonzero count: 457951\n",
      "cnr4_T_C_Avg Nonzero count: 307187\n",
      "short_up_Avg Nonzero count: 307187\n",
      "short_dn_Avg Nonzero count: 307187\n",
      "long_up_corr_Avg Nonzero count: 307187\n",
      "long_dn_corr_Avg Nonzero count: 307187\n"
     ]
    }
   ],
   "source": [
    "basic_stats(data2_fn,1,[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloned_pygem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
