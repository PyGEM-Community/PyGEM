{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code\n",
    "GOAL: take in RAW, not quality controlled data and clean it up. This does NOT include interpolation of missing data points, but does include:\n",
    "1. Compare to sensor health data or logs of known failure periods and remove data in those windows\n",
    "2. Plausible value check. Removes any values that are beyond standard range for each variable\n",
    "3. Remove noise. Higher-level processing but could follow USGS methods\n",
    "4. Compare site-best. If multiple sensors exist for a given data variable, compare values and fill gaps \n",
    "5. Combine data files. If data is separated into multiple datafiles, resample to get on the same time basis (can resample to hourly here, or the lowest resolution of what is available). Combine files with consistent naming into one dataframe for checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_fp = '~/research/climate_data/AWS/Raw/'\n",
    "data_fns = ['SouthGlacier_AWS_HalfHourData.csv','SouthGlacier_AWS_FiveMinData.csv','SouthGlacier_AWS_HealthData.csv']\n",
    "time_vn = 'TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TIMESTAMP', 'RECORD', 'DT', 'TCDT', 'BP', 'BPSC', 'Rain_mm_Tot'], dtype='object')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(AWS_fp+data_fns[0])\n",
    "start = pd.to_datetime(df[time_vn][0])\n",
    "end = pd.to_datetime(df[time_vn][np.shape(df)[0]-1])\n",
    "df = df.set_index(pd.to_datetime(df[time_vn]))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename variables to have consistency\n",
    "\n",
    "*** How to handle when there are multiple columns with the same data? i.e. two SWin terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 variables were not renamed, including:\n",
      "['TIMESTAMP', 'RECORD', 'DT', 'TCDT', 'BPSC', 'Rain_mm_Tot']\n"
     ]
    }
   ],
   "source": [
    "names = {'temp':['site_temp_USGS','temperature','Tair_aws','temp','TA_2.0m','T','AirTC'],\n",
    "            'tp':['Precip_Weighing_Incremental','precipitation','Ptotal_aws','tp','P','Rain_mm_tot'],\n",
    "            'rh':['RelHum','RH','rh','rH','RH_aws','RH_2.0m'],\n",
    "            'SWin':['RadiationIn','SWin','SWin_aws','SW_IN','short_dn_Avg'],\n",
    "            'SWout':['RadiationOut','SWout','SWout_aws','SW_out','SW_OUT','short_up_Avg'],\n",
    "            'LWin':['LWRadiationIn','LWin','LWin_aws','LW_in','LW_IN','long_dn_corr_Avg'],\n",
    "            'LWout':['LWRadiationOut','LWout','LWout_aws','LW_OUT','long_up_corr_Avg'],\n",
    "            'wind':['WindSpeed','wind','Wind','ws_aws','WS','WS_ms_S_WVT'],\n",
    "            'winddir':['VecAvgWindDir','WindDir','Winddir','winddir','WD','WindDir_D1_WVT'],\n",
    "            'sp':['barom','sp','press','Press_aws','Barom','BP'],\n",
    "            'tcc':['cloud_fraction','tcc','CCF','CCF_aws']}\n",
    "# RENAMING\n",
    "drop_vars = []\n",
    "all_vars = ['temp','tp','rh','SWin','SWout','LWin','LWout','wind','winddir','sp','tcc']\n",
    "for var in df.columns.to_numpy():\n",
    "    renamed = False\n",
    "    for var_check in all_vars:\n",
    "        if var in names[var_check]:\n",
    "            df = df.rename(columns={var:var_check})\n",
    "            all_vars.remove(var_check)\n",
    "            renamed = True\n",
    "    if not renamed:\n",
    "        drop_vars.append(var)\n",
    "if len(drop_vars) > 0:\n",
    "    print(len(drop_vars),'variables were not renamed, including:')\n",
    "    print(drop_vars)\n",
    "else:\n",
    "    drop_vars = [0]\n",
    "df = df.drop(columns=drop_vars)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check datatypes to sort out random strings or non-float values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensor malfunctions to remove time periods where the sensor is known to be malfunctioning, according to some indicator (panel temperature, voltage, etc.) and healthy limit for said indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    3085\n",
      "True      251\n",
      "Name: Panel_Temp_Max, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load in health dataset and specify bounds to remove datapoints\n",
    "health_df = pd.read_csv(AWS_fp+data_fns[2],index_col=time_vn)\n",
    "indicator = 'Panel_Temp_Max'\n",
    "healthy_limit = 10\n",
    "unhealthy_idx = health_df[indicator] > healthy_limit\n",
    "# print(unhealthy_idx)\n",
    "print(unhealthy_idx.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plausible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over True    160104\n",
      "Name: sp, dtype: int64 under False    160104\n",
      "Name: sp, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define boundaries for each variable\n",
    "bounds = {'temp':[-40,30],'precip':[0,60],'wind':[0,75],'winddir':[0,360],\n",
    "            'sp':[50,110],'SWin':[0,1600],'SWout':[0,1600],'LWin':[-100,400],'LWout':[-100,400],'rh':[0,100],'tcc':[0,100]}\n",
    "units = {'temp':'C','precip':'mm hr-1','wind':'m s-1','winddir':'deg',\n",
    "            'sp':'kPa','sw':'W m-2','lw':'W m-2','rh':'%','tcc':'%'}\n",
    "# df['sp'] = df['sp'] / 10\n",
    "for var in df.columns:\n",
    "    over = df[var] > bounds[var][1]\n",
    "    under = df[var] < bounds[var][0]\n",
    "    print('over',over.value_counts(),'under',under.value_counts())\n",
    "    df[var] = df[var].mask(over,bounds[var][1])\n",
    "    df[var] = df[var].mask(under,bounds[var][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_half_hour = df.resample('H').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_five_min['sp'] = df_half_hour['sp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_five_min.to_csv('~/research/climate_data/AWS/Raw/southglacier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         wind     winddir      temp         rh      SWout  \\\n",
      "TIMESTAMP                                                                   \n",
      "2006-07-09 15:00:00  2.924545  300.618182  3.542000  76.073636        NaN   \n",
      "2006-07-09 16:00:00  2.235833  289.091667  3.145000  82.265833        NaN   \n",
      "2006-07-09 17:00:00  0.574625  184.580000  3.541875  85.627500        NaN   \n",
      "2006-07-09 18:00:00  0.927150  156.536500  3.652900  88.525000        NaN   \n",
      "2006-07-09 19:00:00  2.803000  231.638333  3.172000  90.466667        NaN   \n",
      "...                       ...         ...       ...        ...        ...   \n",
      "2015-08-27 04:00:00  1.008500   95.405583 -0.140333  86.441667   0.024507   \n",
      "2015-08-27 05:00:00  1.089500  211.322167 -0.499333  87.383333   0.000000   \n",
      "2015-08-27 06:00:00  0.936917  152.792167 -0.473917  89.450000   0.586242   \n",
      "2015-08-27 07:00:00  1.698333  121.810000 -0.029000  87.816667  11.155348   \n",
      "2015-08-27 08:00:00  1.098667  119.483333  0.306333  84.150000  41.789327   \n",
      "\n",
      "                          SWin       LWout        LWin  \n",
      "TIMESTAMP                                               \n",
      "2006-07-09 15:00:00        NaN         NaN         NaN  \n",
      "2006-07-09 16:00:00        NaN         NaN         NaN  \n",
      "2006-07-09 17:00:00        NaN         NaN         NaN  \n",
      "2006-07-09 18:00:00        NaN         NaN         NaN  \n",
      "2006-07-09 19:00:00        NaN         NaN         NaN  \n",
      "...                        ...         ...         ...  \n",
      "2015-08-27 04:00:00   1.659396  271.163942  306.224458  \n",
      "2015-08-27 05:00:00   1.684817  265.136608  304.458600  \n",
      "2015-08-27 06:00:00   1.142668  294.608567  308.534683  \n",
      "2015-08-27 07:00:00   4.948082  303.300950  311.724225  \n",
      "2015-08-27 08:00:00  16.148340  297.249183  313.084700  \n",
      "\n",
      "[80058 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "df_five_min = df.resample('H').mean()\n",
    "print(df_five_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14775/361441679.py:6: DtypeWarning: Columns (11,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_load = pd.read_csv(AWS_fp+fn,encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# def merge_files(fns):\n",
    "fns = data_fns[0:2]\n",
    "df = pd.read_csv(AWS_fp+fns[0],index_col=time_vn)\n",
    "timestep_original = pd.to_datetime(df.index[1]) - pd.to_datetime(df.index[0])\n",
    "for fn in fns[1:]:\n",
    "    df_load = pd.read_csv(AWS_fp+fn,encoding='ISO-8859-1')\n",
    "    timestep_load = pd.to_datetime(df_load.index[1]) - pd.to_datetime(df_load.index[0])\n",
    "    if timestep_load < timestep_original:\n",
    "        print(timestep_load.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stats(fn,header,droprows):\n",
    "    df = pd.read_csv(AWS_fp+fn,header=header,encoding = 'ISO-8859-1')\n",
    "    df = df.drop(droprows,axis=0)\n",
    "    df = df.set_index(time_vn)\n",
    "    for column in df.columns:\n",
    "        print(column,'Nonzero count:',df[column].count(),'          ',df[column].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2729/916621188.py:2: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(AWS_fp+fn,header=header,encoding = 'ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECORD Nonzero count: 956668\n",
      "WS_ms_S_WVT Nonzero count: 956668\n",
      "WindDir_D1_WVT Nonzero count: 956668\n",
      "WindDir_SD1_WVT Nonzero count: 956668\n",
      "WS_ms_Max Nonzero count: 956668\n",
      "AirTC Nonzero count: 956668\n",
      "NR_Wm2_Avg Nonzero count: 956668\n",
      "CNR_Wm2_Avg Nonzero count: 191532\n",
      "RH Nonzero count: 956668\n",
      "SWin_Wm2_Avg Nonzero count: 457951\n",
      "SWout_Wm2_Avg Nonzero count: 457951\n",
      "cnr4_T_C_Avg Nonzero count: 307187\n",
      "short_up_Avg Nonzero count: 307187\n",
      "short_dn_Avg Nonzero count: 307187\n",
      "long_up_corr_Avg Nonzero count: 307187\n",
      "long_dn_corr_Avg Nonzero count: 307187\n"
     ]
    }
   ],
   "source": [
    "basic_stats(data2_fn,1,[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/claire/research/climate_data/AWS/Preprocessed/saintsorlin/saintsorlin_hourly.csv',index_col=0)\n",
    "print(df.index.to_numpy()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloned_pygem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
